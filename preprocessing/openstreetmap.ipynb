{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5f/1gjsvsqj409fll4sv5p42c5c0000gn/T/ipykernel_1674/2725210353.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "from shapely.geometry import Polygon, MultiPolygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_simple_polygon(geom):\n",
    "    if isinstance(geom, Polygon):\n",
    "        return not geom.interiors\n",
    "    elif isinstance(geom, MultiPolygon):\n",
    "        return all(not poly.interiors for poly in geom)\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(dataset_path, column_names, output_path):\n",
    "    batch_size = 100000\n",
    "    lakes_reader = pd.read_csv(\n",
    "        dataset_path,\n",
    "        chunksize=batch_size,\n",
    "        delimiter=\"\\t\",\n",
    "        header=None,\n",
    "        on_bad_lines=\"warn\",\n",
    "        names=column_names,\n",
    "    )\n",
    "\n",
    "    total_rows = 0\n",
    "    for i, chunk in enumerate(lakes_reader):\n",
    "        chunk = chunk.dropna(subset=[\"shape\"])\n",
    "        chunk = chunk[chunk[\"shape\"].str.startswith(\"POLYGON\")].reset_index(drop=True)\n",
    "        chunk = chunk[\n",
    "            chunk[\"shape\"].apply(lambda x: is_simple_polygon(wkt.loads(x)))\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        chunk_df = pd.DataFrame(chunk[\"shape\"])\n",
    "        chunk_df.to_csv(\n",
    "            output_path, mode=(\"w\" if i == 0 else \"a\"), header=None, index=False\n",
    "        )\n",
    "        total_rows += chunk_df.shape[0]\n",
    "        print(f\"Finished chunk {i}\")\n",
    "\n",
    "    print(f\"TOTAL ROWS: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter lakes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished chunk 0\n",
      "Finished chunk 1\n",
      "Finished chunk 2\n",
      "Finished chunk 3\n",
      "Finished chunk 4\n",
      "Finished chunk 5\n",
      "Finished chunk 6\n",
      "Finished chunk 7\n",
      "Finished chunk 8\n",
      "Finished chunk 9\n",
      "Finished chunk 10\n",
      "Finished chunk 11\n",
      "Finished chunk 12\n",
      "Finished chunk 13\n",
      "Finished chunk 14\n",
      "Finished chunk 15\n",
      "Finished chunk 16\n",
      "Finished chunk 17\n",
      "Finished chunk 18\n",
      "Finished chunk 19\n",
      "Finished chunk 20\n",
      "Finished chunk 21\n",
      "Finished chunk 22\n",
      "Finished chunk 23\n",
      "Finished chunk 24\n",
      "Finished chunk 25\n",
      "Finished chunk 26\n",
      "Finished chunk 27\n",
      "Finished chunk 28\n",
      "Finished chunk 29\n",
      "Finished chunk 30\n",
      "Finished chunk 31\n",
      "Finished chunk 32\n",
      "Finished chunk 33\n",
      "Finished chunk 34\n",
      "Finished chunk 35\n",
      "Finished chunk 36\n",
      "Finished chunk 37\n",
      "Finished chunk 38\n",
      "Finished chunk 39\n",
      "Finished chunk 40\n",
      "Finished chunk 41\n",
      "Finished chunk 42\n",
      "Finished chunk 43\n",
      "Finished chunk 44\n",
      "Finished chunk 45\n",
      "Finished chunk 46\n",
      "Finished chunk 47\n",
      "Finished chunk 48\n",
      "Finished chunk 49\n",
      "Finished chunk 50\n",
      "Finished chunk 51\n",
      "Finished chunk 52\n",
      "Finished chunk 53\n",
      "Finished chunk 54\n",
      "Finished chunk 55\n",
      "Finished chunk 56\n",
      "Finished chunk 57\n",
      "Finished chunk 58\n",
      "Finished chunk 59\n",
      "Finished chunk 60\n",
      "Finished chunk 61\n",
      "Finished chunk 62\n",
      "Finished chunk 63\n",
      "Finished chunk 64\n",
      "Finished chunk 65\n",
      "Finished chunk 66\n",
      "Finished chunk 67\n",
      "Finished chunk 68\n",
      "Finished chunk 69\n",
      "Finished chunk 70\n",
      "Finished chunk 71\n",
      "Finished chunk 72\n",
      "Finished chunk 73\n",
      "Finished chunk 74\n",
      "Finished chunk 75\n",
      "Finished chunk 76\n",
      "Finished chunk 77\n",
      "Finished chunk 78\n",
      "Finished chunk 79\n",
      "Finished chunk 80\n",
      "Finished chunk 81\n",
      "Finished chunk 82\n",
      "Finished chunk 83\n",
      "Finished chunk 84\n",
      "TOTAL ROWS: 7460147\n"
     ]
    }
   ],
   "source": [
    "filter_dataset(\n",
    "    dataset_path=\"../../dataset_files/lakes\",\n",
    "    column_names=[\"way_id\", \"shape\", \"tags\"],\n",
    "    output_path=\"../../dataset_files/lakes_filtered\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter parks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished chunk 0\n",
      "Finished chunk 1\n",
      "Finished chunk 2\n",
      "Finished chunk 3\n",
      "Finished chunk 4\n",
      "Finished chunk 5\n",
      "Finished chunk 6\n",
      "Finished chunk 7\n",
      "Finished chunk 8\n",
      "Finished chunk 9\n",
      "Finished chunk 10\n",
      "Finished chunk 11\n",
      "Finished chunk 12\n",
      "Finished chunk 13\n",
      "Finished chunk 14\n",
      "Finished chunk 15\n",
      "Finished chunk 16\n",
      "Finished chunk 17\n",
      "Finished chunk 18\n",
      "Finished chunk 19\n",
      "Finished chunk 20\n",
      "Finished chunk 21\n",
      "Finished chunk 22\n",
      "Finished chunk 23\n",
      "Finished chunk 24\n",
      "Finished chunk 25\n",
      "Finished chunk 26\n",
      "Finished chunk 27\n",
      "Finished chunk 28\n",
      "Finished chunk 29\n",
      "Finished chunk 30\n",
      "Finished chunk 31\n",
      "Finished chunk 32\n",
      "Finished chunk 33\n",
      "Finished chunk 34\n",
      "Finished chunk 35\n",
      "Finished chunk 36\n",
      "Finished chunk 37\n",
      "Finished chunk 38\n",
      "Finished chunk 39\n",
      "Finished chunk 40\n",
      "Finished chunk 41\n",
      "Finished chunk 42\n",
      "Finished chunk 43\n",
      "Finished chunk 44\n",
      "Finished chunk 45\n",
      "Finished chunk 46\n",
      "Finished chunk 47\n",
      "Finished chunk 48\n",
      "Finished chunk 49\n",
      "Finished chunk 50\n",
      "Finished chunk 51\n",
      "Finished chunk 52\n",
      "Finished chunk 53\n",
      "Finished chunk 54\n",
      "Finished chunk 55\n",
      "Finished chunk 56\n",
      "Finished chunk 57\n",
      "Finished chunk 58\n",
      "Finished chunk 59\n",
      "Finished chunk 60\n",
      "Finished chunk 61\n",
      "Finished chunk 62\n",
      "Finished chunk 63\n",
      "Finished chunk 64\n",
      "Finished chunk 65\n",
      "Finished chunk 66\n",
      "Finished chunk 67\n",
      "Finished chunk 68\n",
      "Finished chunk 69\n",
      "Finished chunk 70\n",
      "Finished chunk 71\n",
      "Finished chunk 72\n",
      "Finished chunk 73\n",
      "Finished chunk 74\n",
      "Finished chunk 75\n",
      "Finished chunk 76\n",
      "Finished chunk 77\n",
      "Finished chunk 78\n",
      "Finished chunk 79\n",
      "Finished chunk 80\n",
      "Finished chunk 81\n",
      "Finished chunk 82\n",
      "Finished chunk 83\n",
      "Finished chunk 84\n",
      "Finished chunk 85\n",
      "Finished chunk 86\n",
      "Finished chunk 87\n",
      "Finished chunk 88\n",
      "Finished chunk 89\n",
      "Finished chunk 90\n",
      "Finished chunk 91\n",
      "Finished chunk 92\n",
      "Finished chunk 93\n",
      "Finished chunk 94\n",
      "Finished chunk 95\n",
      "Finished chunk 96\n",
      "Finished chunk 97\n",
      "Finished chunk 98\n",
      "Finished chunk 99\n",
      "TOTAL ROWS: 9747150\n"
     ]
    }
   ],
   "source": [
    "filter_dataset(\n",
    "    dataset_path=\"../../dataset_files/parks\",\n",
    "    column_names=[\"way_id\", \"shape\", \"tags\"],\n",
    "    output_path=\"../../dataset_files/parks_filtered\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create sub - datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read fitlered data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapefile_path = \"continents_geometries/World_Continents\"\n",
    "continents_df = gpd.read_file(shapefile_path)\n",
    "continents_df = continents_df[[\"CONTINENT\", \"geometry\"]]\n",
    "continents = [\"North America\", \"South America\", \"Oceania\", \"Europe\", \"Asia\", \"Africa\"]\n",
    "continents_df = continents_df[continents_df[\"CONTINENT\"].isin(continents)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CONTINENT</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Africa</td>\n",
       "      <td>MULTIPOLYGON (((3950542.075 -2473747.938, 3946...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asia</td>\n",
       "      <td>MULTIPOLYGON (((-20037507.067 10744605.176, -2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Oceania</td>\n",
       "      <td>MULTIPOLYGON (((20037507.067 -1916837.495, 200...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>South America</td>\n",
       "      <td>MULTIPOLYGON (((-7481659.964 -7536755.913, -74...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Europe</td>\n",
       "      <td>MULTIPOLYGON (((2654806.574 4235181.553, 26684...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>North America</td>\n",
       "      <td>MULTIPOLYGON (((-9092406.120 824784.893, -9089...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       CONTINENT                                           geometry\n",
       "0         Africa  MULTIPOLYGON (((3950542.075 -2473747.938, 3946...\n",
       "1           Asia  MULTIPOLYGON (((-20037507.067 10744605.176, -2...\n",
       "3        Oceania  MULTIPOLYGON (((20037507.067 -1916837.495, 200...\n",
       "4  South America  MULTIPOLYGON (((-7481659.964 -7536755.913, -74...\n",
       "6         Europe  MULTIPOLYGON (((2654806.574 4235181.553, 26684...\n",
       "7  North America  MULTIPOLYGON (((-9092406.120 824784.893, -9089..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents_df.to_crs(epsg=4326, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "continent_polygons = {\n",
    "    \"OC\": continents_df[continents_df[\"CONTINENT\"] == \"Oceania\"][\"geometry\"].values[0],\n",
    "    \"AF\": continents_df[continents_df[\"CONTINENT\"] == \"Africa\"][\"geometry\"].values[0],\n",
    "    \"NA\": continents_df[continents_df[\"CONTINENT\"] == \"North America\"][\n",
    "        \"geometry\"\n",
    "    ].values[0],\n",
    "    \"SA\": continents_df[continents_df[\"CONTINENT\"] == \"South America\"][\n",
    "        \"geometry\"\n",
    "    ].values[0],\n",
    "    \"EU\": continents_df[continents_df[\"CONTINENT\"] == \"Europe\"][\"geometry\"].values[0],\n",
    "    \"AS\": continents_df[continents_df[\"CONTINENT\"] == \"Asia\"][\"geometry\"].values[0],\n",
    "}\n",
    "datasets = {\"parks_filtered\": \"O6\"}\n",
    "dataset_paths = {\n",
    "    \"parks_filtered\": \"../../dataset_files/parks_filtered\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_poly(polygon, dataset_nickname):\n",
    "    for continent_name, continent_polygon in continent_polygons.items():\n",
    "        if polygon.intersects(continent_polygon):\n",
    "            return dataset_nickname + continent_name\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing parks_filtered\n",
      "Finished chunk 0\n",
      "Finished chunk 1\n",
      "Finished chunk 2\n",
      "Finished chunk 3\n",
      "Finished chunk 4\n",
      "Finished chunk 5\n",
      "Finished chunk 6\n",
      "Finished chunk 7\n",
      "Finished chunk 8\n",
      "Finished chunk 9\n",
      "Finished chunk 10\n",
      "Finished chunk 11\n",
      "Finished chunk 12\n",
      "Finished chunk 13\n",
      "Finished chunk 14\n",
      "Finished chunk 15\n",
      "Finished chunk 16\n",
      "Finished chunk 17\n"
     ]
    }
   ],
   "source": [
    "for dataset_name, dataset_nickname in datasets.items():\n",
    "    print(f\"Processing {dataset_name}\")\n",
    "    dataset_path = dataset_paths[dataset_name]\n",
    "    batch_size = 100000\n",
    "    reader = pd.read_csv(\n",
    "        dataset_path,\n",
    "        chunksize=batch_size,\n",
    "        on_bad_lines=\"warn\",\n",
    "        header=None,\n",
    "        names=[\"shape\"],\n",
    "    )\n",
    "    for i, chunk in enumerate(reader):\n",
    "        geo_df = gpd.GeoDataFrame(\n",
    "            pd.DataFrame(chunk[\"shape\"].apply(wkt.loads)), geometry=\"shape\"\n",
    "        )\n",
    "        geo_df[\"dataset_name\"] = geo_df[\"shape\"].apply(\n",
    "            group_poly, args=(dataset_nickname,)\n",
    "        )\n",
    "        geo_df = geo_df.dropna()\n",
    "        for combined_dataset_name in geo_df[\"dataset_name\"].unique():\n",
    "            save_path = (\n",
    "                f\"../../dataset_files/OSM_filtered_datasets/{combined_dataset_name}\"\n",
    "            )\n",
    "\n",
    "            cur_dataset_polygons = geo_df[\n",
    "                geo_df[\"dataset_name\"] == combined_dataset_name\n",
    "            ].drop(columns=[\"dataset_name\"])\n",
    "\n",
    "            cur_dataset_polygons.to_csv(\n",
    "                save_path, mode=(\"w\" if i == 0 else \"a\"), header=None, index=False\n",
    "            )\n",
    "\n",
    "        print(f\"Finished chunk {i}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global-counterfactuals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
